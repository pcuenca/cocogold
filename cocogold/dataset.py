# AUTOGENERATED! DO NOT EDIT! File to edit: ../nbs/coco-semantic.ipynb.

# %% auto 0
__all__ = ['Rect', 'class_name', 'get_patch', 'segmented_mask', 'segment_pairs', 'random_square_crop_and_resize',
           'transformed_segments', 'is_contained', 'is_partially_contained', 'semantic_transformed_segments',
           'is_crowd', 'is_valid', 'CocoControlNetDataset']

# %% ../nbs/coco-semantic.ipynb 3
import matplotlib.pyplot as plt
from pycocotools.coco import COCO
from PIL import Image

# %% ../nbs/coco-semantic.ipynb 10
def class_name(class_id, cats):
    for i in range(len(cats)):
        if cats[i]['id'] == class_id:
            return cats[i]['name']
    return "None"

# %% ../nbs/coco-semantic.ipynb 31
import matplotlib.pyplot as plt
import matplotlib.patches as patches
from matplotlib.path import Path
import numpy as np

def get_patch(landmarks, color='white', closed=False):
    contour = landmarks
    ops = [Path.MOVETO] + [Path.LINETO]*(len(contour)-1)
    facecolor = (0, 0, 0, 0)      # Transparent fill color, if open
    if closed:
        contour.append(contour[0])
        ops.append(Path.CLOSEPOLY)
        facecolor = color
    path = Path(contour, ops)
    return patches.PathPatch(path, facecolor=facecolor, edgecolor=color, lw=4)

def segmented_mask(segments, width=512, height=512):
    # Precisely control output image size
    dpi = 72
    fig, ax = plt.subplots(1, figsize=[width/dpi, height/dpi], tight_layout={'pad':0})
    fig.set_dpi(dpi)

    black = np.zeros((height, width, 3))
    ax.imshow(black)

    if not any(isinstance(item, list) for item in segments):
        segments = [segments]
        
    for segment in segments:
        patch = get_patch(segment, closed=True)
#        print(f"Path bounding box: {patch.get_extents()}")
        ax.add_patch(patch)

    plt.axis('off')    
    fig.canvas.draw()
    buffer, (c_width, c_height) = fig.canvas.print_to_buffer()
    assert c_width == width
    assert c_height == height
    
    buffer = np.frombuffer(buffer, np.uint8).reshape((height, width, 4))
    buffer = buffer[:, :, 0:3]
    plt.close(fig)
    return Image.fromarray(buffer)

# %% ../nbs/coco-semantic.ipynb 34
def segment_pairs(coordinates_list):
    """
    Convert a flat list of coordinates into a list of 2D pairs.
    The last item is discarded if the len is odd.
    """
    return list(zip(coordinates_list[::2], coordinates_list[1::2]))

# %% ../nbs/coco-semantic.ipynb 49
from random import randint

def random_square_crop_and_resize(pilimg, size=512):
    """
    Will downsample or upsample as necessary.
    Returns a dict with the image, the crop rect and the scale factor.
    """
    width, height = pilimg.size
    minsize = min(width, height)
    x0 = randint(0, width - height) if width > height else 0
    y0 = randint(0, height - width) if height > width else 0
    pilimg = pilimg.crop((x0, y0, x0 + minsize, y0 + minsize))
    pilimg = pilimg.resize((size, size), resample=Image.LANCZOS)
    return {
        "resized_image": pilimg,
        "crop_rect": (x0, y0, x0+minsize, y0+minsize),
        "scale": float(size)/minsize
    }

# %% ../nbs/coco-semantic.ipynb 57
def transformed_segments(segmentation, crop_rect, scale):
    segments = segment_pairs(segmentation)
    
    def transform_segment(segment):
        x, y = segment
        x = x - crop_rect[0]
        y = y - crop_rect[1]
        return x*scale, y*scale
    
    segments = map(lambda s: transform_segment(s), segments)
    return list(segments)

# %% ../nbs/coco-semantic.ipynb 62
import math
import random
from collections import namedtuple
from functools import partial

Rect = namedtuple("Rect", "x0 y0 x1 y1")

# %% ../nbs/coco-semantic.ipynb 63
def is_contained(annotation, crop_rect):
    """Returns true if the annotation is fully contained inside the crop_rect."""
    crop_x0, crop_y0, crop_x1, crop_y1 = crop_rect
    bbox = annotation["bbox"]
    x0, y0, x1, y1 = (bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3])
    contained =               x0 >= crop_x0
    contained = contained and y0 >= crop_y0
    contained = contained and x1 <= crop_x1
    contained = contained and y1 <= crop_y1
    return contained

# %% ../nbs/coco-semantic.ipynb 64
def is_partially_contained(annotation, crop_rect):
    """
    Returns true if:
    - The bounding boxes overlap at least 60%
    - Or the cropped annotation bbox takes more than 15% of the crop_rect area
    """
    crop = Rect(*crop_rect)
    bbox = annotation["bbox"]
    rect = Rect(bbox[0], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3])
    
    # Using the bbox area instead of the polygon's
    bbox_area = (rect.x1 - rect.x0) * (rect.y1 - rect.y0)
    crop_area = (crop.x1 - crop.x0) * (crop.y1 - crop.y0)
    if bbox_area == 0 or crop_area == 0:
        return False
    
    dx = min(crop.x1, rect.x1) - max(crop.x0, rect.x0)
    dy = min(crop.y1, rect.y1) - max(crop.y0, rect.y0)
    if (dx >= 0) and (dy >= 0):
        if (dx*dy)/bbox_area >= 0.75: return True
        return (dx*dy)/crop_area >= 0.15
    return False

# %% ../nbs/coco-semantic.ipynb 74
def semantic_transformed_segments(annotations, annotation, crop_rect, scale):
    """Return a list of transformerd segments for all the instances
    with the same class of the given annotation."""

    same_class_instances = list(filter(lambda a: a["category_id"] == annotation["category_id"], annotations))
    instances_segments = [s for a in same_class_instances for s in a["segmentation"]]

    return [transformed_segments(s, crop_rect, scale) for s in instances_segments]

# %% ../nbs/coco-semantic.ipynb 91
import torch
from torch.utils.data import Dataset

# %% ../nbs/coco-semantic.ipynb 92
def is_crowd(annotation):
    return "counts" in annotation["segmentation"]

def is_valid(annotation, crop_rect):
    return is_partially_contained(annotation, crop_rect) and not is_crowd(annotation)

# %% ../nbs/coco-semantic.ipynb 93
class CocoControlNetDataset(Dataset):
    """
    A dataset for COCO 2017 semantic segmentation, suitable for Marigold training (hopefully).
    
    Items are a dictionary containing:
    - A random crop of the original image.
    - A segmentation mask for one random category of an instance inside the crop.
    - The class name of the segmentation mask.
    
    Note that results will be different each time you iterate. Larger subjects are selected with higher probability.
    """
    
    def __init__(self, dataset_root, split="val", size=768):
        self.path = dataset_root
        self.split = split
        self.size = size
        annotations_path = f"{self.path}/annotations/instances_{split}2017.json"
        self.coco = COCO(annotations_path)
        self.imgs = self.coco.getImgIds()
        self.cats = self.coco.loadCats(self.coco.getCatIds())
        self._remove_empty()
        
    def _remove_empty(self):
        print("Filtering images with no annotations")
        def has_annotations(img_id):
            ann_ids = self.coco.getAnnIds(img_id)
            return len(ann_ids) > 0
        
        self.imgs = [img for img in self.imgs if has_annotations(img)]
        
    def __len__(self):
        return len(self.imgs)
    
    def resized(self, img_info):
        """Return a tuple resized_image, crop_rect and scale"""
        pilimg = Image.open(f"{self.path}/images/{self.split}/{img_info['file_name']}")    
        resized_dict = random_square_crop_and_resize(pilimg, size=self.size)
        return resized_dict["resized_image"], resized_dict["crop_rect"], resized_dict["scale"]
    
    def __getitem__(self, idx):
        img = self.imgs[idx]
        anns = self.coco.loadAnns(self.coco.getAnnIds(img))
        img_info = self.coco.loadImgs(img)[0]
        
        # Crop, then filter contained and not `iscrowd` annotations, assign probabilities according to area
        # Retry a few times in case our crop does not contain any valid annotations
        for _ in range(5):
            pilimg, crop_rect, scale = self.resized(img_info)
            contained_fn = partial(is_valid, crop_rect=crop_rect)
            contained = list(filter(contained_fn, anns))
            if len(contained) > 0: break
        
        if len(contained) == 0:
            return None
        
        min_area = max_area = contained[0]["area"]
        for annotation in contained[1:]:
            area = annotation["area"]
            min_area = min(area, min_area)
            max_area = max(area, max_area)
        indices = []
        for i, annotation in enumerate(contained):
            area = annotation["area"]
            reps = round(1 + math.log2(area/min_area))
            indices.extend([i] * reps)
        
        annotation = contained[random.choice(indices)]

        # Gather segments from all instances of the same class as the selected choice
        # but consider only valid annotations
        valid_anns = list(filter(contained_fn, anns))
        segments = semantic_transformed_segments(valid_anns, annotation, crop_rect, scale)
        
        # The following would just extract the selected instance
        # segments = transformed_segments(annotation["segmentation"][0], crop_rect, scale)
        
        mask = segmented_mask(segments, width=self.size, height=self.size)
        
        category = class_name(annotation["category_id"], self.cats)
        
        return {
            "key": idx,
            "image": pilimg,
            "mask": mask,
            "class": category,
        }
